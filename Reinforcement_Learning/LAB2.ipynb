{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1de4652",
   "metadata": {},
   "source": [
    "# Reinforcement Learning :\n",
    "\n",
    "## Pre-Lab : Value Iteration (VI)\n",
    "\n",
    "- Implement the V.I algorithm with the Bellman Equation \n",
    "$$ V^{\\pi}(s) = R(s) + \\gamma \\sum_{s'} P_{s\\pi(s)}(s')V^{\\pi}(s') $$\n",
    "\n",
    "- V.I algorithm, for each state compute : \n",
    "$$ V(S) = R(S) + \\gamma\\max_a \\sum_{S'} Psa(S')V(s') $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dce89f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library pythons :\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tkinter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98dc5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the map of 11 cells\n",
    "S = [(0,0), (0,1), (0,2), (0,3),\n",
    "     (1,0),        (1,2), (1,3),\n",
    "     (2,0), (2,1), (2,2), (2,3) ]\n",
    "\n",
    "# All possible transitions\n",
    "S__ = {(0,0): [(0,0), (0,1), (1,0)],\n",
    "       (1,0): [(1,0), (0,0), (2,0)],\n",
    "       (2,0): [(2,0), (1,0), (2,1)],\n",
    "       (0,1): [(0,0), (0,1), (0,2)],\n",
    "       (0,2): [(0,1), (0,2), (0,3), (1,2)],\n",
    "       (2,3): [(2,2), (2,3), (1,3)],\n",
    "       (2,2): [(2,1), (2,2), (2,3), (1,2)],\n",
    "       (2,1): [(2,0), (2,1), (2,2)],\n",
    "       (1,2): [(0,2), (1,2), (2,2), (1,3)],\n",
    "       (1,3): [(1,3)],\n",
    "       (0,3): [(0,3)]}\n",
    "\n",
    "\n",
    "# Init the V matrix\n",
    "V = np.zeros((3,4))\n",
    "\n",
    "# Init the Action set\n",
    "A = ['N', 'S', 'E', 'W']\n",
    "\n",
    "# State transition distribution\n",
    "def Psa(s, a, s_prime):\n",
    "    prob_N = np.array([[0.0, 0.8, 0.0],\n",
    "                       [0.1, 0.0, 0.1],\n",
    "                       [0.0, 0.0, 0.0]])\n",
    "    \n",
    "    prob_S = np.array([[0.0, 0.0, 0.0],\n",
    "                       [0.1, 0.0, 0.1],\n",
    "                       [0.0, 0.8, 0.0]])\n",
    "    \n",
    "    prob_E = np.array([[0.0, 0.1, 0.0],\n",
    "                       [0.0, 0.0, 0.8],\n",
    "                       [0.0, 0.1, 0.0]])\n",
    "    \n",
    "    prob_W = np.array([[0.0, 0.1, 0.0],\n",
    "                       [0.8, 0.0, 0.0],\n",
    "                       [0.0, 0.1, 0.0]])\n",
    "    i = s_prime[0] - s[0] + 1\n",
    "    j = s_prime[1] - s[1] + 1\n",
    "    \n",
    "    # \n",
    "    if (a == 'N'):\n",
    "        return prob_N[i][j]\n",
    "    elif (a == 'S'):\n",
    "        return prob_S[i][j]\n",
    "    elif (a == 'E'):\n",
    "        return prob_E[i][j]\n",
    "    elif (a == 'W'):\n",
    "        return prob_W[i][j]\n",
    "    \n",
    "    \n",
    "# Reward function\n",
    "def R(s):\n",
    "    if (s == (0,3)):\n",
    "        return 1\n",
    "    elif (s == (1,3)):\n",
    "        return -1\n",
    "    else:\n",
    "        return -0.02\n",
    "    \n",
    "# The discount factor\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80758670",
   "metadata": {},
   "source": [
    "### Test of the Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "191ea0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "iterations = 1000\n",
    "\n",
    "while i < iterations:\n",
    "    for state in S:\n",
    "        max_actions = list()\n",
    "        for action in A:\n",
    "            s = 0\n",
    "            for s_next in S__.get(state):\n",
    "                s += Psa(state, action, s_next) * V[s_next[0], s_next[1]]\n",
    "            max_actions.append(s)\n",
    "        V[state[0], state[1]] = R(state) + gamma * max(max_actions)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86b7093",
   "metadata": {},
   "source": [
    "### The Optimal Value :\n",
    "$$ V \\to V^* $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cc2ee47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5204131 ,  0.63331887,  0.82489757,  1.        ],\n",
       "       [ 0.39216717,  0.        ,  0.53431887, -1.        ],\n",
       "       [ 0.32482867,  0.34578052,  0.46184409,  0.24678052]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da8e672",
   "metadata": {},
   "source": [
    "### The Optimal Policy :\n",
    "$$ \\Pi^* = \\argmax_{a} \\sum_{S'} Psa(S')V^*(S')$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4feceffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Policy\n",
    "PI = [['p', 'p', 'p', 'p'],\n",
    "      ['p', 'p', 'p', 'p'],\n",
    "      ['p', 'p', 'p', 'p']]\n",
    "\n",
    "for state in S:\n",
    "    max_actions2 = list()\n",
    "    for action in A:\n",
    "        s = 0\n",
    "        for s_next in S__.get(state):\n",
    "            s += Psa(state, action, s_next) * V[s_next[0], s_next[1]]\n",
    "        max_actions2.append(s)\n",
    "    PI[state[0]][state[1]] = A[np.argmax(max_actions2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25100aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c8323c",
   "metadata": {},
   "source": [
    "### Visualization :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3812132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_arrow(c, a, pX, pY):\n",
    "    if (a == 'N'):\n",
    "        c.create_line([(50+pX,25+pY), (50+pX,75+pY)], fill=\"black\")\n",
    "        c.create_line([(50+pX,25+pY), (25+pX,50+pY)], fill=\"black\")\n",
    "        c.create_line([(50+pX,25+pY), (75+pX,50+pY)], fill=\"black\")\n",
    "    elif (a == 'S'):\n",
    "        c.create_line([(50+pX,25+pY), (50+pX,75+pY)], fill=\"black\")\n",
    "        c.create_line([(50+pX,75+pY), (25+pX,50+pY)], fill=\"black\")\n",
    "        c.create_line([(50+pX,75+pY), (75+pX,50+pY)], fill=\"black\")\n",
    "    elif (a == 'W'):\n",
    "        c.create_line([(25+pX,50+pY), (75+pX,50+pY)], fill=\"black\")\n",
    "        c.create_line([(25+pX,50+pY), (50+pX,25+pY)], fill=\"black\")\n",
    "        c.create_line([(25+pX,50+pY), (50+pX,75+pY)], fill=\"black\")\n",
    "    elif (a == 'E'):\n",
    "        c.create_line([(25+pX,50+pY), (75+pX,50+pY)], fill=\"black\")\n",
    "        c.create_line([(75+pX,50+pY), (50+pX,25+pY)], fill=\"black\")\n",
    "        c.create_line([(75+pX,50+pY), (50+pX,75+pY)], fill=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e310f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "window = Tk()\n",
    "\n",
    "\n",
    "# Canvas\n",
    "canvas = Canvas(window, width=400, height=300, background='white')\n",
    "\n",
    "#\n",
    "w = 400\n",
    "h = 300\n",
    "pX = 0\n",
    "pY = 0\n",
    "\n",
    "#\n",
    "for i in range(0, w+100, 100):\n",
    "    canvas.create_line([(i, 0), (i, h)], fill=\"black\")\n",
    "    \n",
    "for j in range(0, h+100, 100):\n",
    "    canvas.create_line([(0, j), (w, j)], fill=\"black\")\n",
    "    \n",
    "#\n",
    "for i in range(3):\n",
    "    pY = 100 * i\n",
    "    for j in range(4):\n",
    "        pX = 100 * j \n",
    "        if (PI[i][j] == 'p'):\n",
    "            canvas.create_rectangle(200,200,100,100, fill=\"black\")\n",
    "        elif (i == 0 and j == 3):\n",
    "            canvas.create_rectangle(400,100,300,0, fill=\"green\")\n",
    "        elif (i == 1 and j == 3):\n",
    "            canvas.create_rectangle(400,200,300,100, fill=\"red\")\n",
    "        else:\n",
    "            create_arrow(canvas, PI[i][j], pX, pY)\n",
    "\n",
    "#\n",
    "canvas.pack()\n",
    "\n",
    "#\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c06c23a",
   "metadata": {},
   "source": [
    "## Lab : Policy Iteration and Q-Learning :\n",
    "\n",
    "### Functions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c1be163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtains (S_t+1, r_t) from (S_t, a_t)\n",
    "def calculus(s_t, a_t):\n",
    "    # forbidden moves\n",
    "    if (s_t[0] == 2) and a_t == 'S':\n",
    "        return (s_t, R(s_t))\n",
    "    elif (s_t[0] == 0) and a_t == 'N':\n",
    "        return (s_t, R(s_t))\n",
    "    elif (s_t[1] == 0) and a_t == 'W':\n",
    "        return (s_t, R(s_t))\n",
    "    elif (s_t[1] == 3) and a_t == 'E':\n",
    "        return (s_t, R(s_t))\n",
    "    else:\n",
    "        # get s_t+1\n",
    "        prob = list()\n",
    "        states = list()\n",
    "        for s_next in S__.get(s_t):\n",
    "            prob.append(Psa(s_t, a_t, s_next)) # * qTable[S.index(s_next),A.index(a_t)])\n",
    "            states.append(s_next)\n",
    "        return (states[np.argmax(prob)], R(states[np.argmax(prob)]))\n",
    "\n",
    "\n",
    "#\n",
    "def selectNewAction(currentState, e, qTable):\n",
    "    \"\"\"\n",
    "    Chooses a new action according to the epsilon-greedy selection method.\n",
    "    \"\"\"\n",
    "    # Choose a random action with probability epsilon.\n",
    "    if random.random() < e:\n",
    "        return random.randint(0, 3)\n",
    "    # With probability 1-epsilon, choose the action corresponding to the maximum reward.\n",
    "    else:\n",
    "        stateIndex = S.index(currentState)\n",
    "        maxQValue = max(qTable[stateIndex])\n",
    "        # If multiple actions give the maximum value, we randomly choose one of those maximum actions.\n",
    "        maxIndexes = []\n",
    "        for i in range(4):\n",
    "            if qTable[stateIndex][i] == maxQValue:\n",
    "                maxIndexes.append(i)\n",
    "        return maxIndexes[random.randint(0, len(maxIndexes) - 1)]\n",
    "    \n",
    "def playRound(state, action, qTable, alpha):\n",
    "    \"\"\"\n",
    "    Updates the Q-Table.\n",
    "    And make the next state\n",
    "    \"\"\"\n",
    "    # Compute the next state s+1\n",
    "    (S_t_prime, r_t) = calculus(state, action)\n",
    "    \n",
    "    # indexes of Q\n",
    "    index_i = S.index(state)\n",
    "    index_j = A.index(action)\n",
    "        \n",
    "    # Bellman equation\n",
    "    stateIndex = S.index(S_t_prime)\n",
    "    qTable[index_i, index_j] = ((1 - alpha) * qTable[index_i, index_j]) + alpha * (r_t + (gamma*max(qTable[stateIndex])))\n",
    "    \n",
    "    return S_t_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7dbbca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the Q matrix :\n",
    "Q = np.zeros((11,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec73200",
   "metadata": {},
   "source": [
    "### Test of Q-learning algorithm :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5f8b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters :\n",
    "iterations = 2000\n",
    "i = 0\n",
    "epsilon = 0.1\n",
    "alpha = 0.02\n",
    "\n",
    "# algorithm :\n",
    "while i < iterations:\n",
    "    S_0 = random.choice(S)\n",
    "    while (S_0 != (0, 3) and S_0 != (1, 3)):\n",
    "        \n",
    "        # Take a new action\n",
    "        new_action = A[selectNewAction(S_0, epsilon, Q)]\n",
    "        # Move to the next state and update the Q-table\n",
    "        S_0 = playRound(S_0, new_action, Q, alpha)\n",
    "        \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29899e2",
   "metadata": {},
   "source": [
    "### Optimal Q :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3036a0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09002123,  0.09535825,  0.93923223,  0.16262049],\n",
       "       [ 0.14936367,  0.2544454 ,  0.96999423,  0.15205409],\n",
       "       [ 0.47100999,  0.57710393,  1.        ,  0.56012748],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.84027417,  0.00805161,  0.03695877,  0.07555059],\n",
       "       [ 0.96999996,  0.29639626, -0.34574419,  0.36378655],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.0516614 ,  0.04181905,  0.80918201,  0.01434457],\n",
       "       [ 0.04408353,  0.11310364,  0.90822188,  0.07155445],\n",
       "       [ 0.94029468,  0.1978928 ,  0.15654984,  0.15832965],\n",
       "       [-0.14923698,  0.08190376,  0.02024618,  0.85971711]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1227df7",
   "metadata": {},
   "source": [
    "### Get Optimal Policy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa978cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Policy\n",
    "PI_2 = [['p', 'p', 'p', 'p'],\n",
    "        ['p', 'p', 'p', 'p'],\n",
    "        ['p', 'p', 'p', 'p']]\n",
    "\n",
    "\n",
    "for state in S:\n",
    "    k = S.index(state)\n",
    "    PI_2[state[0]][state[1]] = A[np.argmax(Q[k,:])]\n",
    "\n",
    "#\n",
    "PI_2[0][3] = \"+1\"\n",
    "PI_2[1][3] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70aaedbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "window = Tk()\n",
    "\n",
    "\n",
    "# Canvas\n",
    "canvas = Canvas(window, width=400, height=300, background='white')\n",
    "\n",
    "#\n",
    "w = 400\n",
    "h = 300\n",
    "pX = 0\n",
    "pY = 0\n",
    "\n",
    "#\n",
    "for i in range(0, w+100, 100):\n",
    "    canvas.create_line([(i, 0), (i, h)], fill=\"black\")\n",
    "    \n",
    "for j in range(0, h+100, 100):\n",
    "    canvas.create_line([(0, j), (w, j)], fill=\"black\")\n",
    "    \n",
    "#\n",
    "for i in range(3):\n",
    "    pY = 100 * i\n",
    "    for j in range(4):\n",
    "        pX = 100 * j \n",
    "        if (PI_2[i][j] == 'p'):\n",
    "            canvas.create_rectangle(200,200,100,100, fill=\"black\")\n",
    "        elif (i == 0 and j == 3):\n",
    "            canvas.create_rectangle(400,100,300,0, fill=\"green\")\n",
    "        elif (i == 1 and j == 3):\n",
    "            canvas.create_rectangle(400,200,300,100, fill=\"red\")\n",
    "        else:\n",
    "            create_arrow(canvas, PI_2[i][j], pX, pY)\n",
    "\n",
    "#\n",
    "canvas.pack()\n",
    "\n",
    "#\n",
    "window.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
